#
# cluster-api template
#   openstack:
#     without LB
#     with existing network/subnet
#     with corporate http proxy
#     HA ctrlplane with kube-vip
#
#   prereq:
#     port and ip must be created before and outside cluster-api
#
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OpenStackMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      configDrive: true
      flavor: ${OPENSTACK_NODE_MACHINE_FLAVOR}
      image:
        filter:
          name: ${OPENSTACK_IMAGE_NAME}
      sshKeyName: ${OPENSTACK_SSH_KEY_NAME}
      ports:
        - network:
            filter:
              name: ${OPENSTACK_NETWORK_NAME}
          fixedIPs:
            - subnet:
                filter:
                  name: ${OPENSTACK_SUBNET_NAME}
          securityGroups:   ##  debug : add other SG
            - filter:
                name: k8s-cluster-${NAMESPACE}-${CLUSTER_NAME}-secgroup-worker
            - filter:
                name: allow-http  ## Must created outside cluster-api, if external LB redirect to nodeport on worker node
            - filter:
                name: allow-ssh  ## Must created outside cluster-api (for debug only)
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OpenStackMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      configDrive: true
      flavor: ${OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR}
      image:
        filter:
          name: ${OPENSTACK_IMAGE_NAME}
      sshKeyName: ${OPENSTACK_SSH_KEY_NAME}
      ports:
        - network:
            filter:
              name: ${OPENSTACK_NETWORK_NAME}
          fixedIPs:
            - subnet:
                filter:
                  name: ${OPENSTACK_SUBNET_NAME}
          securityGroups:
            - filter:
                name: k8s-cluster-${NAMESPACE}-${CLUSTER_NAME}-secgroup-controlplane
            - filter:
                name: allow-ssh  ## Must created outside cluster-api (for debug only)
          allowedAddressPairs:
            - ipAddress: ${API_SERVER_FIXED_IP}  ## Port and IP Must be created outside cluster-api
#
#      rootVolume:
#        sizeGiB: 15
#        type: __DEFAULT__
#      additionalBlockDevices:
#        - name: data
#          sizeGiB: 10
#          storage:
#            type: Volume
#            volume:
#              type: __DEFAULT__
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OpenStackCluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  bastion:
    enabled: false
    spec:
      flavor: ${OPENSTACK_BASTION_MACHINE_FLAVOR}
      image:
        filter:
          name: ${OPENSTACK_BASTION_IMAGE_NAME}
      sshKeyName: ${OPENSTACK_SSH_KEY_NAME}
  # externalnet enable is needed by bastion
  disableExternalNetwork: true
  disableAPIServerFloatingIP: true
  apiServerFixedIP: ${API_SERVER_FIXED_IP}  ## Port and IP Must be created outside cluster-api
  externalNetwork:
    filter:
      name: ${OPENSTACK_EXTERNAL_NETWORK_NAME}
  controlPlaneAvailabilityZones: ${OPENSTACK_AVAILABILITY_ZONES:=["AZ1","AZ2"]}
  router:
    filter:
      name: ${OPENSTACK_ROUTER_NAME}
  network:
    filter:
      name: ${OPENSTACK_NETWORK_NAME}
  subnets:
    - filter:
        name: ${OPENSTACK_SUBNET_NAME}
  identityRef:
    cloudName: ${OPENSTACK_CLOUD}
    name: ${CLUSTER_NAME}-cloud-config
  managedSecurityGroups:
    # open all inside cluster for Debug only
    allowAllInClusterTraffic: true
    allNodesSecurityGroupRules:
      - description: Created by cluster-api-provider-openstack - BGP (calico)
        direction: ingress
        etherType: IPv4
        name: BGP (Calico)
        portRangeMax: 179
        portRangeMin: 179
        protocol: tcp
        remoteManagedGroups:
          - controlplane
          - worker
      - description: Created by cluster-api-provider-openstack - IP-in-IP (calico)
        direction: ingress
        etherType: IPv4
        name: IP-in-IP (calico)
        protocol: "4"
        remoteManagedGroups:
          - controlplane
          - worker
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT:=1}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      clusterName: ${CLUSTER_NAME}
      # failureDomain: ${OPENSTACK_FAILURE_DOMAIN}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: OpenStackMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: ${NAMESPACE}
spec:
  kubeadmConfigSpec:
    preKubeadmCommands:
      - echo debug-pre > /tmp/out
      - mkdir -p /etc/pre-kubeadm-commands
      - for script in $(find /etc/pre-kubeadm-commands/ -name '*.sh' -type f | sort);
        do echo "Running script $script"; "$script"; done
      - systemctl daemon-reload
      - systemctl restart containerd
    postKubeadmCommands:
      - echo debug-post >> /tmp/out
      - mkdir -p /etc/post-kubeadm-commands
      - for script in $(find /etc/post-kubeadm-commands/ -name '*.sh' -type f | sort);
        do echo "Running script $script"; "$script"; done
    files:
      - path: /etc/kubernetes/manifests/kube-vip.yaml
        owner: "root:root"
        permissions: "0644"
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            name: kube-vip
            namespace: kube-system
          spec:
            containers:
            - args:
              - manager
              env:
              - name: vip_arp
                value: "true"
              - name: port
                value: "6443"
              - name: vip_nodename
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
              - name: vip_interface
                value: ${VIP_INTERFACE:=ens3}
              - name: vip_cidr
                value: "32"
              - name: vip_subnet
                value: /32
              - name: dns_mode
                value: "false"
              - name: cp_enable
                value: "true"
              - name: cp_namespace
                value: kube-system
              - name: svc_enable
                value: "true"
              - name: svc_leasename
                value: plndr-svcs-lock
              - name: vip_leaderelection
                value: "true"
              - name: vip_leasename
                value: plndr-cp-lock
              - name: vip_leaseduration
                value: "15"
              - name: vip_renewdeadline
                value: "10"
              - name: vip_retryperiod
                value: "2"
              - name: address
                value: ${API_SERVER_FIXED_IP}
              - name: prometheus_server
                value: :2112
              image: ghcr.io/kube-vip/kube-vip:v0.8.0
              imagePullPolicy: IfNotPresent
              name: kube-vip
              resources: {}
              securityContext:
                capabilities:
                  add:
                  - NET_ADMIN
                  - NET_RAW
              volumeMounts:
              - mountPath: /etc/kubernetes/admin.conf
                name: kubeconfig
              # bug kube 1.29
              - mountPath: /etc/hosts
                name: etchosts
            hostNetwork: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/admin.conf
              name: kubeconfig
            - hostPath:
                path: /etc/kube-vip.hosts
                type: File
              name: etchosts
          status: {}
      - content: 127.0.0.1 localhost kubernetes
        owner: root:root
        path: /etc/kube-vip.hosts
        permissions: "0644"
      - content: |
          #!/bin/bash
          # Copyright 2020 The Kubernetes Authors.
          #
          # Licensed under the Apache License, Version 2.0 (the "License");
          # you may not use this file except in compliance with the License.
          # You may obtain a copy of the License at
          #
          #     http://www.apache.org/licenses/LICENSE-2.0
          #
          # Unless required by applicable law or agreed to in writing, software
          # distributed under the License is distributed on an "AS IS" BASIS,
          # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          # See the License for the specific language governing permissions and
          # limitations under the License.
          set -e
          # Configure the workaround required for kubeadm init with kube-vip:
          # xref: https://github.com/kube-vip/kube-vip/issues/684
          # Nothing to do for kubernetes < v1.29
          KUBEADM_MINOR="$(kubeadm version -o short | cut -d '.' -f 2)"
          if [[ "$KUBEADM_MINOR" -lt "29" ]]; then
            exit 0
          fi
          IS_KUBEADM_INIT="false"
          # cloud-init kubeadm init
          if [[ -f /run/kubeadm/kubeadm.yaml ]]; then
            IS_KUBEADM_INIT="true"
          fi
          # ignition kubeadm init
          if [[ -f /etc/kubeadm.sh ]] && grep -q -e "kubeadm init" /etc/kubeadm.sh; then
            IS_KUBEADM_INIT="true"
          fi
          if [[ "$IS_KUBEADM_INIT" == "true" ]]; then
            sed -i 's#path: /etc/kubernetes/admin.conf#path: /etc/kubernetes/super-admin.conf#' \
              /etc/kubernetes/manifests/kube-vip.yaml
          fi
        owner: root:root
        path: /etc/pre-kubeadm-commands/50-kube-vip-prepare.sh
        permissions: "0700"
      - content: |
          #!/bin/bash
          set -e
          # Configure the workaround required for kubeadm init with kube-vip:
          # xref: https://github.com/kube-vip/kube-vip/issues/684
          # Nothing to do for kubernetes < v1.29
          KUBEADM_MINOR="$(kubeadm version -o short | cut -d '.' -f 2)"
          if [[ "$KUBEADM_MINOR" -lt "29" ]]; then
            exit 0
          fi
          IS_KUBEADM_INIT="false"
          # cloud-init kubeadm init
          if [[ -f /run/kubeadm/kubeadm.yaml ]]; then
            IS_KUBEADM_INIT="true"
          fi
          # ignition kubeadm init
          if [[ -f /etc/kubeadm.sh ]] && grep -q -e "kubeadm init" /etc/kubeadm.sh; then
            IS_KUBEADM_INIT="true"
          fi
          if [[ "$IS_KUBEADM_INIT" == "true" ]]; then
            sed -i 's#path: /etc/kubernetes/super-admin.conf#path: /etc/kubernetes/admin.conf#' \
              /etc/kubernetes/manifests/kube-vip.yaml
          fi
        owner: root:root
        path: /etc/post-kubeadm-commands/50-kube-vip-restore.sh
        permissions: "0700"
      - path: /etc/apt/apt.conf.d/99proxy
        owner: "root:root"
        permissions: "0644"
        content: |
          Acquire {
            http::Proxy "${HTTP_PROXY}";
            https::Proxy "${HTTP_PROXY}";
            }
      - path: /etc/systemd/system/containerd.service.d/http-proxy.conf
        owner: "root:root"
        permissions: "0644"
        content: |
          [Service]
          Environment="HTTP_PROXY=${HTTP_PROXY}"
          Environment="HTTPS_PROXY=${HTTP_PROXY}"
          Environment="NO_PROXY=.svc,.svc.cluster,.svc.cluster.local,127.0.0.0/8,10.96.0.0/12,172.16.0.0/16,192.168.0.0/16"
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: openstack:///'{{ instance_id }}'
        name: '{{ local_hostname }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: openstack:///'{{ instance_id }}'
        name: '{{ local_hostname }}'
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: OpenStackMachineTemplate
      name: ${CLUSTER_NAME}-control-plane
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=1}
  version: ${KUBERNETES_VERSION}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      preKubeadmCommands:
        - echo debug-pre > /tmp/out
        - systemctl daemon-reload
        - systemctl restart containerd
      postKubeadmCommands:
        - echo debug-post >> /tmp/out
      files:
        - path: /etc/apt/apt.conf.d/99proxy
          owner: "root:root"
          permissions: "0644"
          content: |
            Acquire {
              http::Proxy "${HTTP_PROXY}";
              https::Proxy "${HTTP_PROXY}";
              }
        - path: /etc/systemd/system/containerd.service.d/http-proxy.conf
          owner: "root:root"
          permissions: "0644"
          content: |
            [Service]
            Environment="HTTP_PROXY=${HTTP_PROXY}"
            Environment="HTTPS_PROXY=${HTTP_PROXY}"
            Environment="NO_PROXY=.svc,.svc.cluster,.svc.cluster.local,127.0.0.0/8,10.96.0.0/12,172.16.0.0/16,192.168.0.0/16"
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            provider-id: openstack:///'{{ instance_id }}'
          name: '{{ local_hostname }}'
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ${POD_CIDR:=["172.16.0.0/16"]}
    services:
      cidrBlocks: ${SERVICE_CIDR:=["10.96.0.0/12"]}
    serviceDomain: ${SERVICE_DOMAIN:="cluster.local"}
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: OpenStackCluster
    name: ${CLUSTER_NAME}
