#
# cluster-api template
#   openstack:
#     without LB
#     with managed subnet
#     with corporate http proxy
#
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OpenStackMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      configDrive: true
### Affinity/anti affinity
#      serverGroup:
#        filter:
#          name: ${OPENSTACK_NODE_SERVER_GROUP}  ## must exists outside cluster-api
      flavor: ${OPENSTACK_NODE_MACHINE_FLAVOR}
      image:
        filter:
          name: ${OPENSTACK_IMAGE_NAME}
      sshKeyName: ${OPENSTACK_SSH_KEY_NAME}
      ports:
        - network:
            filter:
               name: k8s-clusterapi-cluster-${NAMESPACE}-${CLUSTER_NAME}
          fixedIPs:
            - subnet:
                filter:
                  name: k8s-clusterapi-cluster-${NAMESPACE}-${CLUSTER_NAME}
          securityGroups:   ##  debug : add other SG
            - filter:
                name: k8s-cluster-${NAMESPACE}-${CLUSTER_NAME}-secgroup-worker
            - filter:
                name: allow-http  ## Must created outside cluster-api, if external LB redirect to nodeport on worker node
            - filter:
                name: allow-ssh  ## Must created outside cluster-api (for debug only)
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OpenStackMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      configDrive: true
### Affinity/anti affinity
#      serverGroup:
#        filter:
#          name: ${OPENSTACK_CONTROL_PLANE_SERVER_GROUP}  ## must exists outside cluster-api

      flavor: ${OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR}
      image:
        filter:
          name: ${OPENSTACK_IMAGE_NAME}
      sshKeyName: ${OPENSTACK_SSH_KEY_NAME}
      ports:
        - network:
            filter:
              name: k8s-clusterapi-cluster-${NAMESPACE}-${CLUSTER_NAME}
          fixedIPs:
            - subnet:
                filter:
                  name: k8s-clusterapi-cluster-${NAMESPACE}-${CLUSTER_NAME}
          securityGroups:
            - filter:
                name: k8s-cluster-${NAMESPACE}-${CLUSTER_NAME}-secgroup-controlplane
            - filter:
                name: allow-ssh  ## Must created outside cluster-api (for debug only)
#
#      rootVolume:
#        sizeGiB: 15
#        type: __DEFAULT__
#      additionalBlockDevices:
#        - name: data
#          sizeGiB: 10
#          storage:
#            type: Volume
#            volume:
#              type: __DEFAULT__
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OpenStackCluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  bastion:
    enabled: false
    spec:
      flavor: ${OPENSTACK_BASTION_MACHINE_FLAVOR}
      image:
        filter:
          name: ${OPENSTACK_BASTION_IMAGE_NAME}
      sshKeyName: ${OPENSTACK_SSH_KEY_NAME}
  ## api with floating IP
  disableExternalNetwork: false
  externalNetwork:
    filter:
      name: ${OPENSTACK_EXTERNAL_NETWORK_NAME}
  controlPlaneAvailabilityZones: ${OPENSTACK_AVAILABILITY_ZONES:=["AZ1","AZ2"]}
  router:
    filter:
      name: ${OPENSTACK_ROUTER_NAME}
  managedSubnets:
    - cidr: ${OPENSTACK_MANAGED_SUBNET_CIDR:="192.168.102.0/24"}
#    dnsNameservers:
#    - ${OPENSTACK_DNS_NAMESERVERS}
#    allocationPools:
#      - start: 192.168.102.100
#        end: 192.168.102.252
  identityRef:
    cloudName: ${OPENSTACK_CLOUD}
    name: ${CLUSTER_NAME}-cloud-config
  managedSecurityGroups:
    # open all inside cluster for Debug only
    allowAllInClusterTraffic: true
    allNodesSecurityGroupRules:
      - description: Created by cluster-api-provider-openstack - BGP (calico)
        direction: ingress
        etherType: IPv4
        name: BGP (Calico)
        portRangeMax: 179
        portRangeMin: 179
        protocol: tcp
        remoteManagedGroups:
          - controlplane
          - worker
      - description: Created by cluster-api-provider-openstack - IP-in-IP (calico)
        direction: ingress
        etherType: IPv4
        name: IP-in-IP (calico)
        protocol: "4"
        remoteManagedGroups:
          - controlplane
          - worker
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT:=1}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      clusterName: ${CLUSTER_NAME}
      # failureDomain: ${OPENSTACK_FAILURE_DOMAIN}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: OpenStackMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: ${NAMESPACE}
spec:
  kubeadmConfigSpec:
    preKubeadmCommands:
      - echo debug-pre > /tmp/out
      - mkdir -p /etc/pre-kubeadm-commands
      - for script in $(find /etc/pre-kubeadm-commands/ -name '*.sh' -type f | sort);
        do echo "Running script $script"; "$script"; done
    postKubeadmCommands:
      - echo debug-post >> /tmp/out
      - mkdir -p /etc/post-kubeadm-commands
      - for script in $(find /etc/post-kubeadm-commands/ -name '*.sh' -type f | sort);
        do echo "Running script $script"; "$script"; done
    files:
      - content: |
          #!/bin/bash
          #
          # pre-kubadm-scripts
          #
          set -e
          # apply containerd config before kubeadm
          systemctl daemon-reload
          systemctl restart containerd
        owner: root:root
        path: /etc/pre-kubeadm-commands/10-containerd-restart.sh
        permissions: "0700"
      - path: /etc/apt/apt.conf.d/99proxy
        owner: "root:root"
        permissions: "0644"
        content: |
          Acquire {
            http::Proxy "${HTTP_PROXY}";
            https::Proxy "${HTTP_PROXY}";
            }
      - path: /etc/systemd/system/containerd.service.d/http-proxy.conf
        owner: "root:root"
        permissions: "0644"
        content: |
          [Service]
          Environment="HTTP_PROXY=${HTTP_PROXY}"
          Environment="HTTPS_PROXY=${HTTP_PROXY}"
          Environment="NO_PROXY=.svc,.svc.cluster,.svc.cluster.local,127.0.0.0/8,10.96.0.0/12,172.16.0.0/16,192.168.0.0/16"
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: openstack:///'{{ instance_id }}'
        name: '{{ local_hostname }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: openstack:///'{{ instance_id }}'
        name: '{{ local_hostname }}'
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: OpenStackMachineTemplate
      name: ${CLUSTER_NAME}-control-plane
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=1}
  version: ${KUBERNETES_VERSION}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      preKubeadmCommands:
        - echo debug-pre > /tmp/out
        - mkdir -p /etc/pre-kubeadm-commands
        - for script in $(find /etc/pre-kubeadm-commands/ -name '*.sh' -type f | sort);
          do echo "Running script $script"; "$script"; done
      postKubeadmCommands:
        - echo debug-post >> /tmp/out
        - mkdir -p /etc/post-kubeadm-commands
        - for script in $(find /etc/post-kubeadm-commands/ -name '*.sh' -type f | sort);
          do echo "Running script $script"; "$script"; done
      files:
        - content: |
            #!/bin/bash
            #
            # pre-kubadm-scripts
            #
            set -e
            # apply containerd config before kubeadm
            systemctl daemon-reload
            systemctl restart containerd
          owner: root:root
          path: /etc/pre-kubeadm-commands/10-containerd-restart.sh
          permissions: "0700"
        - path: /etc/apt/apt.conf.d/99proxy
          owner: "root:root"
          permissions: "0644"
          content: |
            Acquire {
              http::Proxy "${HTTP_PROXY}";
              https::Proxy "${HTTP_PROXY}";
              }
        - path: /etc/systemd/system/containerd.service.d/http-proxy.conf
          owner: "root:root"
          permissions: "0644"
          content: |
            [Service]
            Environment="HTTP_PROXY=${HTTP_PROXY}"
            Environment="HTTPS_PROXY=${HTTP_PROXY}"
            Environment="NO_PROXY=.svc,.svc.cluster,.svc.cluster.local,127.0.0.0/8,10.96.0.0/12,172.16.0.0/16,192.168.0.0/16"
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            provider-id: openstack:///'{{ instance_id }}'
          name: '{{ local_hostname }}'
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ${POD_CIDR:=["172.16.0.0/16"]}
    services:
      cidrBlocks: ${SERVICE_CIDR:=["10.96.0.0/12"]}
    serviceDomain: ${SERVICE_DOMAIN:="cluster.local"}
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: OpenStackCluster
    name: ${CLUSTER_NAME}
