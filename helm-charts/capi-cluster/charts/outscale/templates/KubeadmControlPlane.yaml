---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: {{ .Values.global.clusterName }}-control-plane
spec:
  kubeadmConfigSpec:
    verbosity: 5
    clusterConfiguration:
      apiServer:
        timeoutForControlPlane: 20m
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
#      etcd:
#        local:
#          extraArgs:
#            listen-client-urls: {{`'https://{{ ds.meta_data.local_ipv4 }}:2379'`}}
#            election-timeout: "5000" ## debug
#            heartbeat-interval: "500"
#            snapshot-count: "5000"
### use external etcd volume dir
#           quota-backend-bytes: "8589934592"
#          dataDir: /var/lib/etcddisk/etcd ## We Change the etcd data dir when moving to dedicated volume (because of "dir is not empty")
    initConfiguration:
#      localAPIEndpoint:
#        advertiseAddress: {{`'{{ ds.meta_data.local_ipv4 }}'`}}
#        bindPort: 6443
      {{ if and .Values.global.addons.cni.enabled ( eq .Values.global.addons.cni.type "cilium" ) -}}
      # cilium replace kube-proxy
      skipPhases:
        - addon/kube-proxy
      {{ end -}}
      nodeRegistration:
        name: {{`'{{ ds.meta_data.local_hostname }}'`}}
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: {{`'aws:///{{ ds.meta_data.placement.availability_zone }}/{{ ds.meta_data.instance_id }}'`}}
    joinConfiguration:
      nodeRegistration:
        name: {{`'{{ ds.meta_data.local_hostname }}'`}}
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: {{`'aws:///{{ ds.meta_data.placement.availability_zone }}/{{ ds.meta_data.instance_id }}'`}}
# cloud-init
##  Add external volume for etcd
#    diskSetup:
##      partitions:
##      - device: /dev/vdb
##        layout: true
##        overwrite: false
##        tableType: gpt
#      filesystems:
#      - device: /dev/vdb
#        extraOpts:
#        - -E
#        - lazy_itable_init=1,lazy_journal_init=1
#        filesystem: ext4
#        label: etcd_disk
#    mounts:
#      - - LABEL=etcd_disk
#        - /var/lib/etcddisk
    preKubeadmCommands:
      - mkdir -p /etc/pre-kubeadm-commands
      - for script in $(find /etc/pre-kubeadm-commands/ -name '*.sh' -type f | sort);
        do echo "Running script $script"; "$script"; done
    postKubeadmCommands:
      - mkdir -p /etc/post-kubeadm-commands
      - for script in $(find /etc/post-kubeadm-commands/ -name '*.sh' -type f | sort);
        do echo "Running script $script"; "$script"; done
    files:
      - content: |
          #!/bin/bash
          #
          # pre-kubadm-scripts
          #
          set -e
          # apply containerd config before kubeadm
          echo "## restart containerd"
          systemctl daemon-reload
          systemctl restart containerd
        owner: root:root
        path: /etc/pre-kubeadm-commands/10-containerd-restart.sh.disable
        permissions: "0700"
      - content: |
          #!/bin/bash
          grep '^      limits:' /etc/kubernetes/manifests/kube-apiserver.yaml >/dev/null 2>&1 && exit 0
          MEM=$(free -m | grep '^Mem:' | awk '{print $2;}')
          CPU=$(grep '^processor' /proc/cpuinfo | wc -l)
          sed -i "/^ *requests:/i\      limits:\n        memory: $((10+3*$MEM/4))M\n        cpu: $((750*$CPU))m" /etc/kubernetes/manifests/kube-apiserver.yaml
          sed -i "/^ *requests:/a\        memory: 512M" /etc/kubernetes/manifests/kube-apiserver.yaml
          sync
          systemctl restart kubelet
        owner: root:root
        path: /etc/post-kubeadm-commands/10-tweak-kubeapi-memlimit.sh.disable
        permissions: "0700"
      - path: /etc/apt/apt.conf.d/99proxy
        owner: "root:root"
        permissions: "0644"
        content: |
          Acquire {
            {{- if .Values.global.http_proxy }}
            http::Proxy {{ .Values.global.http_proxy | quote }};
            {{- end }}
            {{- if .Values.global.https_proxy }}
            https::Proxy {{ .Values.global.https_proxy | quote }};
            {{- end }}
            }
      - path: /etc/systemd/system/containerd.service.d/http-proxy.conf
        owner: "root:root"
        permissions: "0644"
        content: |
          [Service]
          {{- if .Values.global.http_proxy }}
          Environment="HTTP_PROXY={{ .Values.global.http_proxy }}"
          {{- end }}
          {{- if .Values.global.https_proxy }}
          Environment="HTTPS_PROXY={{ .Values.global.https_proxy }}"
          {{- end }}
          {{- if .Values.global.no_proxy }}
          Environment="NO_PROXY=.svc,.svc.cluster,.svc.cluster.local,127.0.0.0/8,192.168.0.0/16,{{ .Values.global.pods.cidrBlocks }},{{ .Values.global.services.cidrBlocks }}"
          {{- end }}
      - content: |
          #!/bin/bash
          set +e
          # Define the retry function
          wait_and_retry() {
            local retries="$1"
            local wait="$2"
            local command="$3"

            $command
            local exit_code=$?

            if [[ $exit_code -ne 0 && $retries -gt 0 ]]; then
              echo "# Wait $wait before retrying $retries : $command"
              sleep $wait
              wait_and_retry $(($retries - 1)) $wait "$command"
            else
              return $exit_code
            fi
          }
          # wait endpoint is up
          endpoint=""
          [[ -f /run/kubeadm/kubeadm.yaml ]] && endpoint=$(grep "controlPlaneEndpoint" /run/kubeadm/kubeadm.yaml|awk ' {print $2 }' |sed -e 's/ //g;s/:.*//g')
          [[ -f  /run/kubeadm/kubeadm-join-config.yaml ]] && endpoint=$(grep "apiServerEndpoint" /run/kubeadm/kubeadm-join-config.yaml |awk ' {print $2 }' |sed -e 's/ //g;s/:.*//g')
          [[ -n "$endpoint" ]] && wait_and_retry 360 10 "host $endpoint"
          # wait external access is up
          external_url="github.com"
          [[ -n "$external_url" ]] && wait_and_retry 360 10 "host $external_url"
        owner: root:root
        path: /etc/pre-kubeadm-commands/00-wait_endpoint.sh
        permissions: "0755"
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: OscMachineTemplate
      name: {{ .Values.global.clusterName }}-control-plane-{{ .Values.controlplane.version }}
  replicas: {{ .Values.controlplane.replicas | default 1 }}
  version: {{ .Values.controlplane.version }}
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 1
    type: RollingUpdate
